## yitu

### Case1: 全服务容量工程

解决的问题： 借鉴 Google SRE USE原则设计，实现成本与稳定性的平衡，服务扩容缩容有依据。峰值利用率维持在 80 ～ 90% 之间。度量全服务容量工程以后，公有云月支出成本降低 15% 。

我做了什么：

1. 监控系统，告警指向性明确。云资源 和 k8s 资源纬度各自有设计。云资源基于公有云云监控，服务监控基于自建的 prometheus-operator；
2. 扩容系统。自建 k8s 集群通过自开发的 ansible 集群管理脚手架工程，批量节点的上下线，实现效果，5分钟节点就绪，10分钟承接生产流量。

其中涉及的环节：

1. 服务发布验证阶段，度量微服务系统中单个服务容量，以及在该容量表现下的计算资源开销；
2. 服务上线后，通过单服务容量 \* 就绪服务服务数量，得出一类服务的容量，度量 US ，合并错误率度量指标；
3. PrometheusRules 观测服务情况，接入告警通知体系；
4. 运维流程。与运营约定流程，触发容量告警，扩容服务，预算额度等；
5. 在利用率不满足，服务缩容，云资源退订，均衡成本；

### Case2：自建 k8s 集群节点管理脚手架工程

解决的问题：

- 自建 k8s 集群创建；
- Dev/Staging/Prod 多个自建 k8s 集群，根据 CPU/GPU 等不同机型，批量做节点上下线；
- 降低节点管理公差，提升工作效率，优化运维工作体验；节点 5 分钟就绪，10 分钟承接生产流量；

其中涉及的环节：

1. 通过 roles 将 playbook 区分为基线、docker 安装，gpu机型 Nvidia-docker2等环节，k8s 依赖组件，集群加入等动作。复用 roles 实现节点初始化、节点上线、节点打标等操作；

2. 多个集群的管理，使用组变量；

### Case3: 监控体系

| 监控对象 | 监控工具                        | 暴露方式                                                     |
| -------- | ------------------------------- | ------------------------------------------------------------ |
| 云资源   | 云监控                          | 云自带                                                       |
| yitu服务 | Prometheus/Grafana/Alertmanager | 业务metrics暴露，如 spring-boot-actuator<br />开源 exporter 暴露 metrics， |

### Case4: 基于 helm-charts 方式的产物发布仓库

定义 helm-charts 发布物，定义发布工作流程；

到 lanma 时期，因为仅采用单机 Docker compose 方式，发布仓库维护成 compose 文件；



## lanma

### Case1: 公司IT架构基础

按物理划分：办公室（IT网络），公有云（aws/阿里云），数据中心（核心基建/GPU训练机）；

按工作内容划分：

- IT整体
  - 整体网络设计（分解为办公室，数据中心，公有云多 region）；
  - 基础设施规约（操作系统，docker-ce 主线版本，nvidia-docker2 主线版本，Docker 服务交维方式，Docker 运行网络模式）；
  - IT 事件管理流程（后移交给 IT）；
  - 办公室与数据中心，办公室与阿里云 VPC 之间的 VPN 落地；
  - 财务方向（预算/决算，资源申请交付流程）；
- 公有云服务治理 
  - 元数据设计，定义 sla， project，区分服务等级以及成本归属，从而归属资源组，资源管理/权限/成本多处使用；
  - VPC/网络设计，大方向上区分生产VPC，开发测试VPC；
  - 访问控制管理，用户/程序使用子账户，框定权限，自助使用；
- 其他：
  - 核心基建，如高可用 gitlab；
  - 可观测体系：
    - 阿里云日志服务，采集容器标准输出，分域查询；
    - 阿里云Prometheus，采集暴露 metrics，Grafana 可视化，统一告警体系；
  - ansible 固化 Linux 标装工艺、跳板机增减用户等运维动作；
  - 发版流程，服务上线流程，数据修改协定与落地；

### Case2: 容器化

背景：按照公司规划，使用容器作为服务交付的主要载体；大多数同事之前未接触过容器，手生。

我的做法：

1. 做选型：容器版本，启停方式，compose 文件定义，容器网络定义，容器网络模型选型；
2. 给案例：维护公司内部的 awsome-compose 代码仓库，提供日常使用的容器网络，nginx/mysql/redis等常用的中间件 compose 文件；运维组件全部使用容器方式交付；
3. 做培训：偏重 Dockerfile 的构成，打包容器镜像。在交维阶段，接收的 compose 范式，组织形式。容器的基础知识，提供 GPT 的 Prompt，做中学。
4. 跟服务：从CI， Dockerfile，Docker build，compose.yaml 文件一直到上线；
5. 发布工具：通过 ansible 发布脚手架；

达成的效果：目前所有服务全容器化。

   
